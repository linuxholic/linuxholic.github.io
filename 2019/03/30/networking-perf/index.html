<!DOCTYPE html>












  


<html class="theme-next muse use-motion" lang>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
























<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/css/main.css?v=7.0.1">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.0.1">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.0.1">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.0.1">


  <link rel="mask-icon" href="/images/logo.svg?v=7.0.1" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '7.0.1',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="主题 基于 linux 内核，机器是如何接受数据包的？ 数据包从网卡到应用层会经过若干个组件，如何针对性的进行监控和调优？  基本原则 理论上，只需要在网络栈的各个层次监控 丢包率，就可以很快的定位系统当前的瓶颈； 这个时候如果只是参考网上的一份”最优“ sysctl 配置，往往不能达到很好的效果； 调优的前提是，我们需要有清晰的可监控指标来验证实际的效果。  概述数据包从抵达网卡开始，一路到达套">
<meta property="og:type" content="article">
<meta property="og:title" content="networking-perf">
<meta property="og:url" content="http://yoursite.com/2019/03/30/networking-perf/index.html">
<meta property="og:site_name" content="linuxholic&#39;s blog">
<meta property="og:description" content="主题 基于 linux 内核，机器是如何接受数据包的？ 数据包从网卡到应用层会经过若干个组件，如何针对性的进行监控和调优？  基本原则 理论上，只需要在网络栈的各个层次监控 丢包率，就可以很快的定位系统当前的瓶颈； 这个时候如果只是参考网上的一份”最优“ sysctl 配置，往往不能达到很好的效果； 调优的前提是，我们需要有清晰的可监控指标来验证实际的效果。  概述数据包从抵达网卡开始，一路到达套">
<meta property="og:locale" content="default">
<meta property="og:updated_time" content="2019-03-30T09:37:40.675Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="networking-perf">
<meta name="twitter:description" content="主题 基于 linux 内核，机器是如何接受数据包的？ 数据包从网卡到应用层会经过若干个组件，如何针对性的进行监控和调优？  基本原则 理论上，只需要在网络栈的各个层次监控 丢包率，就可以很快的定位系统当前的瓶颈； 这个时候如果只是参考网上的一份”最优“ sysctl 配置，往往不能达到很好的效果； 调优的前提是，我们需要有清晰的可监控指标来验证实际的效果。  概述数据包从抵达网卡开始，一路到达套">





  
  
  <link rel="canonical" href="http://yoursite.com/2019/03/30/networking-perf/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>networking-perf | linuxholic's blog</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">linuxholic's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/30/networking-perf/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="linuxholic">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="linuxholic's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">networking-perf

              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-03-30 16:43:29 / Modified: 17:37:40" itemprop="dateCreated datePublished" datetime="2019-03-30T16:43:29+08:00">2019-03-30</time>
            

            
              

              
            
          </span>

          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="主题"><a href="#主题" class="headerlink" title="主题"></a>主题</h1><ul>
<li>基于 linux 内核，机器是如何接受数据包的？</li>
<li>数据包从网卡到应用层会经过若干个组件，如何针对性的进行监控和调优？</li>
</ul>
<h1 id="基本原则"><a href="#基本原则" class="headerlink" title="基本原则"></a>基本原则</h1><ul>
<li>理论上，只需要在网络栈的各个层次监控 <strong>丢包率</strong>，就可以很快的定位系统当前的瓶颈；</li>
<li>这个时候如果只是参考网上的一份”最优“ sysctl 配置，往往不能达到很好的效果；</li>
<li>调优的前提是，我们需要有清晰的可监控指标来验证实际的效果。</li>
</ul>
<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>数据包从抵达网卡开始，一路到达套接字的 receive buffer：</p>
<ul>
<li>驱动加载和初始化</li>
<li>数据包到达网卡控制器(NIC)</li>
<li>数据包被复制到内核空间（ DMA -&gt; ring buffer ）</li>
<li>产生硬件中断，通知系统数据可读</li>
<li>驱动调用 NAPI 激活 poll 循环（如果该循环处于休眠状态）</li>
<li>ksoftirpd 调用驱动注册的 poll 函数，读取 ring buffer 中的数据包</li>
<li>ring buffer 对应的内存区域被解除映射（ memory region unmapped ）</li>
<li>数据包被封装为 <code>skb</code> 结构体，准备传递到上层协议栈</li>
<li>如果开启网卡多队列，数据帧会被负载均衡到多个 CPU 进行处理</li>
<li>数据帧经由队列，递交上层协议栈</li>
<li>协议栈处理（ IP -&gt; UDP/TCP ）</li>
<li>数据被填充到套接字的 receive buffer</li>
</ul>
<a id="more"></a>
<h1 id="详述"><a href="#详述" class="headerlink" title="详述"></a>详述</h1><p>我们有必要先了解驱动层的工作机制，这样才能更好的理解后半部分的内容</p>
<h2 id="网卡驱动"><a href="#网卡驱动" class="headerlink" title="网卡驱动"></a>网卡驱动</h2><h3 id="PCI-注册"><a href="#PCI-注册" class="headerlink" title="PCI 注册"></a>PCI 注册</h3><p>设备加载时，内核会调用驱动通过 <code>module_init</code> 注册的初始化函数</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *  igb_init_module - Driver Registration Routine</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *  igb_init_module is the first routine called when the driver is</span></span><br><span class="line"><span class="comment"> *  loaded. All it does is register with the PCI subsystem.</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">int</span> __<span class="function">init <span class="title">igb_init_module</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">int</span> ret;</span><br><span class="line">  pr_info(<span class="string">"%s - version %s\n"</span>, igb_driver_string, igb_driver_version);</span><br><span class="line">  pr_info(<span class="string">"%s\n"</span>, igb_copyright);</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* ... */</span></span><br><span class="line"></span><br><span class="line">  ret = pci_register_driver(&amp;igb_driver);</span><br><span class="line">  <span class="keyword">return</span> ret;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">module_init(igb_init_module);</span><br></pre></td></tr></table></figure>
<p>其中 <code>pci_register_driver</code> 具体实现了注册到内核  <strong>PCI子系统</strong> 的任务</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">struct</span> <span class="title">pci_driver</span> <span class="title">igb_driver</span> = &#123;</span></span><br><span class="line">  .name     = igb_driver_name,</span><br><span class="line">  .id_table = igb_pci_tbl,</span><br><span class="line">  .probe    = igb_probe,</span><br><span class="line">  .remove   = igb_remove,</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* ... */</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>其中：</p>
<ul>
<li><code>igb_pci_tbl</code> 定义了一个 table，包含 driver 可以控制的 Device ID 列表；</li>
<li><code>igb_probe</code> 执行一些前期准备工作（激活设备，请求 DMA，注册各种控制函数等）</li>
</ul>
<h3 id="设备初始化"><a href="#设备初始化" class="headerlink" title="设备初始化"></a>设备初始化</h3><p><code>igb_probe</code> 除了一些 PCI 相关的工作，更重要的是网络相关的内容：</p>
<ul>
<li>注册结构体 <code>net_device_ops</code></li>
<li>注册 <code>ethtool</code> 的一系列操作</li>
<li>从 NIC 读取 MAC 地址</li>
<li>设置 <code>net_device</code> 的各种功能标志位</li>
<li>等等</li>
</ul>
<p><code>net_device_ops</code> 包含一系列函数指针，用来控制网卡的各种动作</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="class"><span class="keyword">struct</span> <span class="title">net_device_ops</span> <span class="title">igb_netdev_ops</span> = &#123;</span></span><br><span class="line">  .ndo_open               = igb_open,</span><br><span class="line">  .ndo_stop               = igb_close,</span><br><span class="line">  .ndo_start_xmit         = igb_xmit_frame,</span><br><span class="line">  .ndo_get_stats64        = igb_get_stats64,</span><br><span class="line">  .ndo_set_rx_mode        = igb_set_rx_mode,</span><br><span class="line">  .ndo_set_mac_address    = igb_set_mac,</span><br><span class="line">  .ndo_change_mtu         = igb_change_mtu,</span><br><span class="line">  .ndo_do_ioctl           = igb_ioctl,</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* ... */</span></span><br></pre></td></tr></table></figure>
<p><code>ethtool</code> 对应一个命令行工具，可以用来获取和设置网卡的各项参数配置；需要注意的是，<code>ethtool</code> 工具并不是直接和网卡驱动交互，而是通过 <code>ioctl</code> 经由内核来和网卡交互。</p>
<h4 id="NOTE：IRQ-和-NAPI"><a href="#NOTE：IRQ-和-NAPI" class="headerlink" title="NOTE：IRQ 和 NAPI"></a>NOTE：IRQ 和 NAPI</h4><p>数据包到达内核空间之后，网卡需要通过 irq 通知内核；在网络流量很大的情况下，就会导致大量的中断产生，CPU 忙于处理中断，就可能导致用户任务被延迟处理；为了减少 irq 产生的次数，引入了 NAPI 的机制，允许驱动注册 poll 函数来读取数据帧。下面是 NAPI 的一般流程：</p>
<ol>
<li>驱动开启 NAPI，但是一开始处于未激活状态</li>
<li>数据包到达并被 NIC 通过 DMA 拷贝到内存</li>
<li>NIC 产生中断，触发驱动注册的中断处理函数</li>
<li>驱动通过 softirq 激活 NAPI 子系统；这时会有专门的内核线程开始收割数据帧</li>
<li>驱动关闭 NIC 中断，防止 NAPI 子系统在收割数据帧时被再次中断</li>
<li>数据帧处理结束之后，NAPI 子系统挂起，并重新激活 NIC 中断信号</li>
<li>重复执行上述第二步</li>
</ol>
<p>可以看出，通过驱动注册的 poll 函数，NAPI 能够在一次性处理大量数据帧，而不是中断一次处理一次，大大提高了效率。poll 函数注册到 NAPI 子系统，通常发生在驱动初始化的阶段。</p>
<p>PS：多队列的情况下，每个 rx / tx 队列都对应一个 poll 循环，通过参数 <code>struct napi_struct</code> 区分不同的队列（ poll 函数是同一个，通过参数控制作用到不同的队列上）。</p>
<h3 id="网卡启动"><a href="#网卡启动" class="headerlink" title="网卡启动"></a>网卡启动</h3><p><code>net_device_ops-&gt;ndo_open</code> 对应了网卡启动时调用的函数，比如当我们执行 <code>ifconfig eth0 up</code> 的时候。</p>
<ol>
<li>分配 rx / tx 队列内存</li>
<li>开启 NAPI</li>
<li>注册中断处理函数</li>
<li>开启中断</li>
<li>等等</li>
</ol>
<p>大部分现代 NIC 都是通过 DMA 直接将数据帧写入到内核空间，为了达到这一目的，NIC 使用了类似队列的模型，底层数据结构基于环形buffer，也就是我们通常所说的 <code>ring buffer</code>。所以驱动需要向内核申请内存，并把相应的内存地址通知到网卡，这样 NIC 就可以知道数据帧写入到哪里。</p>
<p>考虑到单个 <code>ring buffer</code> 只能被单个 CPU 处理，并且大小有限，现代 NIC 引入了多队列的机制，可以同时写入多个内存区域，从而使用多个 CPU 并行处理数据帧。我们可以通过 <code>ethtool</code> 工具查看和修改网卡多队列的相关设置：</p>
<ul>
<li>调整队列的数目以及大小</li>
<li>调整队列的权重</li>
<li>调整用于分发数据帧到多个队列的哈希函数</li>
</ul>
<p>虽然 poll 函数是在驱动初始化的时候注册的，但是直到网卡启动才会为每个队列开启 NAPI。</p>
<p>注册中断处理函数时需要注意，网卡支持的中断产生方式有多种：MSI-X, MSI, legacy interrupt。驱动需要逐个尝试，设置对应的处理函数。这里优先选择 MSI-X，尤其是多队列的情况，因为可以针对每个队列设置独立的硬件中断，从而充分利用多 CPU 并行处理。</p>
<ul>
<li>MSI/MSI-X: 使用 in-band 数据模拟中断，无需额外芯片引脚</li>
<li>legacy interrupt：使用专用中断引脚，触发 out-of-band 控制信号</li>
</ul>
<p>激活中断的方式取决于硬件，一般是通过写入网卡的特殊寄存器实现：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">igb_irq_enable</span><span class="params">(struct igb_adapter *adapter)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* ... */</span></span><br><span class="line"></span><br><span class="line">    wr32(E1000_IMS, IMS_ENABLE_MASK | E1000_IMS_DRSTA);</span><br><span class="line">    wr32(E1000_IAM, IMS_ENABLE_MASK | E1000_IMS_DRSTA);</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* ... */</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>至此，网卡已经就绪，随时可以接收网络数据。</p>
<h3 id="网卡监控"><a href="#网卡监控" class="headerlink" title="网卡监控"></a>网卡监控</h3><p>监控网卡有多种方式，各自提供不同的粒度和复杂度。</p>
<ul>
<li><code>ethtool -S</code></li>
<li><code>sysfs</code></li>
<li><code>/proc/net/dev</code></li>
</ul>
<p><code>ethtool</code> 的使用方式如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ethtool -S eth0</span><br><span class="line">NIC statistics:</span><br><span class="line">     rx_packets: 597028087</span><br><span class="line">     tx_packets: 5924278060</span><br><span class="line">     rx_bytes: 112643393747</span><br><span class="line">     tx_bytes: 990080156714</span><br><span class="line">     rx_broadcast: 96</span><br><span class="line">     tx_broadcast: 116</span><br><span class="line">     rx_multicast: 20294528</span><br><span class="line">     ....</span><br></pre></td></tr></table></figure>
<p>对上述数据进行监控其实比较困难。虽然拿到这些数据很简单，但是没有统一的标准对这些字段进行定义。不同的驱动程序，甚至相同驱动程序的不同版本，对同样含义的指标可能有不一样的字段名称。</p>
<p>一般来说，我们只需要关注类似 <code>drop</code>,<code>buffer</code>,<code>miss</code> 这样的指标，但是我们仍然需要通过驱动程序代码来确定这些指标的真实来历。有些指标经过了软件层面的累加（比如内存不足发生的次数），有些指标则是直接从网卡寄存器读取出来的。对于这样的寄存器值，我们还需要参考网卡的 data sheet 来确定其真实含义；因为 <code>ethtool</code> 给出的很多指标字段具有误导性。</p>
<p><code>sysfs</code> 提供的指标相比 <code>ethtool</code> 更高阶一些，比如这样：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cat /sys/class/net/eth0/statistics/rx_dropped</span><br><span class="line">2</span><br></pre></td></tr></table></figure>
<p>不同的指标被分割放到了不同的文件里，比如 <code>collisions</code>,<code>rx_dropped</code>,<code>rx_errors</code> 等等；</p>
<p>类似的，这些指标也取决于具体的驱动程序，每个指标的含义，什么时候累加，来自哪里等等；比如同样一种类型的错误，有些驱动算作 <code>drop</code>，有些驱动则算到 <code>miss</code> 里。</p>
<p><code>/proc/net/dev</code> 提供的数据更为概括性，类似这样：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ cat /proc/net/dev</span><br><span class="line">Inter-|   Receive                                                |  Transmit</span><br><span class="line"> face |bytes    packets errs drop fifo frame compressed multicast|bytes    packets errs drop fifo colls carrier compressed</span><br><span class="line">  eth0: 110346752214 597737500    0    2    0     0          0  20963860 990024805984 6066582604    0    0    0     0       0          0</span><br><span class="line">    lo: 428349463836 1579868535    0    0    0     0          0         0 428349463836 1579868535    0    0    0     0       0          0</span><br></pre></td></tr></table></figure>
<p>这个文件提供的指标是 <code>sysfs</code> 的子集，可以提供一个大概的参考；同样的，我们需要通过驱动源码来确定这些值累加的时机，地点和原因。</p>
<h3 id="网卡调优"><a href="#网卡调优" class="headerlink" title="网卡调优"></a>网卡调优</h3><p>使用 <code>ethtool</code> 工具，我们可以：</p>
<ul>
<li>检查当前使用的队列数目</li>
<li>调整队列数量</li>
<li>调整队列大小</li>
<li>调整队列权重</li>
<li>调整哈希字段</li>
<li><code>ntuple filtering</code> 控制特定数据帧到指定队列</li>
</ul>
<p>举个例子，对于一个监听 80 端口的 web server 来说：</p>
<ul>
<li>web server 绑定到了 CPU2</li>
<li>某个网卡接收队列的中断信号，被指定到 CPU2 处理</li>
<li>目的端口为 80 的 tcp 数据帧被 <em>过滤</em> 到 CPU2</li>
<li>这样的话，从数据帧到达网卡开始，一直到应用层，80 端口的所有流量就都被 CPU2 处理了</li>
<li>缓存命中率和网络延迟都会有改善</li>
</ul>
<h2 id="软中断"><a href="#软中断" class="headerlink" title="软中断"></a>软中断</h2><ul>
<li>内核延时任务机制</li>
<li>内核线程 <code>ksoftirqd</code></li>
<li>监控 <code>/proc/softirqs</code></li>
</ul>
<p>概括来说，<code>softirq</code> 用来在中断处理函数之外执行代码。考虑到在中断处理函数执行期间，一般会屏蔽掉中断信号；那么一旦中断处理耗时越长，中断信号丢失的概率就会越大。所以，对于任何需要长时间运行的代码，有必要延迟到中断处理环境之外执行，从而让中断处理尽快结束，并重新激活中断信号。</p>
<p>整个 <code>softirq</code> 子系统由一系列内核线程组成，每个 CPU 对应一个内核线程。在 <code>top</code> 命令里，我们可以发现有类似 <code>ksoftirqd/0</code> 这样的内核线程，就对应于 CPU0 的 <code>softirq</code> 线程。其他内核子系统（比如网络）可以通过 <code>open_softirq</code> 注册处理函数，也就达到了把任务交给 <code>softirq</code> 执行的目的。</p>
<p><code>ksoftirqd</code> 线程在内核启动期间产生：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">struct</span> <span class="title">smp_hotplug_thread</span> <span class="title">softirq_threads</span> = &#123;</span></span><br><span class="line">  .store              = &amp;ksoftirqd,</span><br><span class="line">  .thread_should_run  = ksoftirqd_should_run,</span><br><span class="line">  .thread_fn          = run_ksoftirqd,</span><br><span class="line">  .thread_comm        = <span class="string">"ksoftirqd/%u"</span>,</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> __<span class="function">init <span class="keyword">int</span> <span class="title">spawn_ksoftirqd</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  register_cpu_notifier(&amp;cpu_nfb);</span><br><span class="line"></span><br><span class="line">  BUG_ON(smpboot_register_percpu_thread(&amp;softirq_threads));</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line">early_initcall(spawn_ksoftirqd);</span><br></pre></td></tr></table></figure>
<p>可以看出，针对每个 CPU 产生独立的线程，并且线程名根据 cpu 数目编号；另外还注册了两个执行函数 <code>ksoftirqd_should_run</code> 和 <code>run_ksoftirqd</code>，前者用来判断是否有待处理的软中断，如果有，则执行后者。</p>
<p><code>run_ksoftirqd</code> 实际是对 <code>__do_softirq</code> 的简单封装，后者执行了一些值得注意的操作：</p>
<ul>
<li>检查是否有软中断等待处理</li>
<li>记录执行时间 - 用来计算耗时？</li>
<li>累加执行次数</li>
<li>执行软中断处理函数（通过 <code>open_softirq</code> 注册）</li>
</ul>
<p>PS：通过工具查看 CPU 使用率的时候，<code>softirq</code>/<code>si</code> 部分就对应于延迟任务执行的耗时。</p>
<p>通过 <code>/proc/softirqs</code> 文件，我们可以监控各类事件触发软中断的频率：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ cat /proc/softirqs</span><br><span class="line">                    CPU0       CPU1       CPU2       CPU3</span><br><span class="line">          HI:          0          0          0          0</span><br><span class="line">       TIMER: 2831512516 1337085411 1103326083 1423923272</span><br><span class="line">      NET_TX:   15774435     779806     733217     749512</span><br><span class="line">      NET_RX: 1671622615 1257853535 2088429526 2674732223</span><br><span class="line">       BLOCK: 1800253852    1466177    1791366     634534</span><br><span class="line">BLOCK_IOPOLL:          0          0          0          0</span><br><span class="line">     TASKLET:         25          0          0          0</span><br><span class="line">       SCHED: 2642378225 1711756029  629040543  682215771</span><br><span class="line">     HRTIMER:    2547911    2046898    1558136    1521176</span><br><span class="line">         RCU: 2056528783 4231862865 3545088730  844379888</span><br></pre></td></tr></table></figure>
<p>目前我们只需要关注 <code>NET_RX</code> 这一行，对应网络收包触发的软中断。可以看出，不同的 CPU 处理的软中断数量并不一样，如果相差悬殊，我们可以通过网卡多队列进行调优。</p>
<h2 id="Linux-网络栈"><a href="#Linux-网络栈" class="headerlink" title="Linux 网络栈"></a>Linux 网络栈</h2><ul>
<li>初始化</li>
<li>数据接收</li>
<li>数据处理</li>
<li>GRO - 数据包合并，需要应用层介入<ul>
<li>提供数据包合并的前提条件</li>
<li>以及停止合并或者合并完成的边界条件</li>
</ul>
</li>
</ul>
<h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><p>执行初始化函数 <code>net_dev_init</code>，基于当前 cpu 数目创建一系列 <code>struct softnet_data</code>。这些结构体保存了很多重要的指针：</p>
<ul>
<li>NAPI 结构体，用来注册到当前 CPU</li>
<li>backlog 队列</li>
<li>处理权重 - <code>processing weight</code></li>
<li>LRO 结构体</li>
<li>RPS 配置</li>
<li>等等</li>
</ul>
<p>此外，初始化函数还分别注册了数据接收和发送的软中断处理函数：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">int</span> __<span class="function">init <span class="title">net_dev_init</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="comment">/* ... */</span></span><br><span class="line"></span><br><span class="line">  open_softirq(NET_TX_SOFTIRQ, net_tx_action);</span><br><span class="line">  open_softirq(NET_RX_SOFTIRQ, net_rx_action);</span><br><span class="line"></span><br><span class="line"> <span class="comment">/* ... */</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="数据接收"><a href="#数据接收" class="headerlink" title="数据接收"></a>数据接收</h3><p>假设 rx queue 有足够的可用描述符，数据帧通过 DMA 写入到 RAM，网卡紧接着触发了中断信号（内核会为每个设备分配一个中断号；对于 MSI-X 则是绑定到 rx queue 的中断号）</p>
<h4 id="中断处理函数"><a href="#中断处理函数" class="headerlink" title="中断处理函数"></a>中断处理函数</h4><p>一般来说，中断处理函数应该尽量延迟任务到中断环境之外执行，因为中断处理期间会屏蔽后续的中断信号。<br>下面这个是 MSI-X 的中断处理函数：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> irqreturn_t <span class="title">igb_msix_ring</span><span class="params">(<span class="keyword">int</span> irq, <span class="keyword">void</span> *data)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="class"><span class="keyword">struct</span> <span class="title">igb_q_vector</span> *<span class="title">q_vector</span> = <span class="title">data</span>;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/* Write the ITR value calculated from the previous interrupt. */</span></span><br><span class="line">  igb_write_itr(q_vector);</span><br><span class="line"></span><br><span class="line">  napi_schedule(&amp;q_vector-&gt;napi);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> IRQ_HANDLED;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>够短吧！这里只做了两步操作：第一步更新硬件寄存器，第二步激活 NAPI 处理循环。</p>
<h4 id="NAPI-napi-schedule"><a href="#NAPI-napi-schedule" class="headerlink" title="NAPI napi_schedule"></a>NAPI <code>napi_schedule</code></h4><p>引入 NAPI 的唯一目的，就是在无需 NIC 中断触发的情况”收割“数据帧。之前提到过，NAPI 的 poll 循环是被 NIC 的硬件中断 <strong>激活</strong> 的；也就是说，NAPI 一直处于 stand by 的模式，直到被首个数据帧触发的硬件中断启动。另外，某些情况下 NAPI 也会被关闭，并等待下一次 NIC 中断触发。</p>
<p><code>napi_schedule</code> 定义在头文件中，只是对 <code>__napi_schedule</code> 的简单封装：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * __napi_schedule - schedule for receive</span></span><br><span class="line"><span class="comment"> * @n: entry to schedule</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * The entry's receive function will be scheduled to run</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">void</span> __napi_schedule(struct napi_struct *n)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="keyword">unsigned</span> <span class="keyword">long</span> flags;</span><br><span class="line"></span><br><span class="line">  local_irq_save(flags);</span><br><span class="line">  ____napi_schedule(&amp;__get_cpu_var(softnet_data), n);</span><br><span class="line">  local_irq_restore(flags);</span><br><span class="line">&#125;</span><br><span class="line">EXPORT_SYMBOL(__napi_schedule);</span><br></pre></td></tr></table></figure>
<p>需要关注的是 <code>__get_cpu_var</code> 这个函数，拿到了注册到当前 CPU 上的 <code>softnet_data</code> 结构体；这个结构体和 <code>struct napi_struct</code> 一起传给了 <code>____napi_schedule</code> 函数：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* Called with irq disabled */</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">inline</span> <span class="keyword">void</span> ____napi_schedule(struct softnet_data *sd,</span><br><span class="line">                                     struct napi_struct *napi)</span><br><span class="line">&#123;</span><br><span class="line">  list_add_tail(&amp;napi-&gt;poll_list, &amp;sd-&gt;poll_list);</span><br><span class="line">  __raise_softirq_irqoff(NET_RX_SOFTIRQ);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里主要做了两个事情：</p>
<ol>
<li>把 <code>struct napi_struct</code> 追加到 <code>softnet_data</code> 的 <code>pool_list</code> 列表；</li>
<li>触发 <code>NET_RX_SOFTIRQ</code> 软中断</li>
</ol>
<p>对于第一点，<code>struct napi_struct</code> 存放了网卡队列对应的内存地址等信息，所以需要传递过去；另外，需要一个列表的原因是，可能存在多个网卡，或者网卡多队列的情况；对于第二点，对应处理函数 <code>net_rx_action</code> 最终会调用驱动程序当初注册的 <code>poll</code> 函数，从而处理数据。</p>
<blockquote>
<p>为什么需要绑定中断处理到特定 CPU ？</p>
</blockquote>
<p>目前为止看到的延迟任务执行都用到了当前 CPU 的数据结构，也就是说，中断处理函数交出去的任务，都绑定到了当前 CPU 上，后续这些任务也就仍然在这个 CPU 上执行。所以通过指定处理中断的 CPU，也就确保了后续 <code>softirq</code> 在相同的 CPU 上执行。</p>
<h4 id="监控数据接收"><a href="#监控数据接收" class="headerlink" title="监控数据接收"></a>监控数据接收</h4><p>需要注意的是，硬件中断的变化并不能完全反应数据帧处理的变化，因为驱动程序一般会在 NAPI 执行期间屏蔽硬件中断。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ cat /proc/interrupts</span><br><span class="line">            CPU0       CPU1       CPU2       CPU3</span><br><span class="line">   0:         46          0          0          0 IR-IO-APIC-edge      timer</span><br><span class="line">   1:          3          0          0          0 IR-IO-APIC-edge      i8042</span><br><span class="line">  30: 3361234770          0          0          0 IR-IO-APIC-fasteoi   aacraid</span><br><span class="line">  64:          0          0          0          0 DMAR_MSI-edge      dmar0</span><br><span class="line">  65:          1          0          0          0 IR-PCI-MSI-edge      eth0</span><br><span class="line">  66:  863649703          0          0          0 IR-PCI-MSI-edge      eth0-TxRx-0</span><br><span class="line">  67:  986285573          0          0          0 IR-PCI-MSI-edge      eth0-TxRx-1</span><br><span class="line">  68:         45          0          0          0 IR-PCI-MSI-edge      eth0-TxRx-2</span><br><span class="line">  69:        394          0          0          0 IR-PCI-MSI-edge      eth0-TxRx-3</span><br><span class="line"> NMI:    9729927    4008190    3068645    3375402  Non-maskable interrupts</span><br><span class="line"> LOC: 2913290785 1585321306 1495872829 1803524526  Local timer interrupts</span><br></pre></td></tr></table></figure>
<p>除了 NAPI 的影响，硬件自身中断合并的机制也会影响这里的数据变化。所以这个文件表现出的中断数量和变化频率，并不能反映实际的数据帧接收量和处理量。为了得到更全面的指标，我们还需要进一步监控 <code>/proc/softirqs</code> 以及 <code>/proc</code> 下的其他文件。</p>
<p>那这个文件的还有啥用？可以用来验证网卡多队列是否生效，每个队列是不是被独立的 CPU 处理。</p>
<h4 id="调优数据接收"><a href="#调优数据接收" class="headerlink" title="调优数据接收"></a>调优数据接收</h4><ul>
<li>中断合并</li>
<li>中断亲和性</li>
</ul>
<blockquote>
<p>Interrupt coalescing</p>
</blockquote>
<p>中断合并用来阻止发送中断信号到 CPU，直到积累了一定数量的待处理事件，以此避免中断风暴，也可以一定程度上改善吞吐量和延迟。<code>ethtool</code> 工具对此也提供了支持：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ethtool -c eth0</span><br><span class="line">Coalesce parameters <span class="keyword">for</span> eth0:</span><br><span class="line">Adaptive RX: off  TX: off</span><br><span class="line">stats-block-usecs: 0</span><br><span class="line">sample-interval: 0</span><br><span class="line">pkt-rate-low: 0</span><br><span class="line">pkt-rate-high: 0</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>友情提醒：并不是每个驱动都会实现 <code>ethtool</code> 的全部设置项。对于驱动不支持的配置项，对应的配置值都会被直接忽略掉，也就不会起作用咯。</p>
<p>值得一提的一个选项是，自适应中断合并。这个功能一般在硬件层面实现，但需要驱动程序的配合才能真正启用。启用这个功能的效果很诱人：流量低峰期降低延时，流量高峰期提升吞吐。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ethtool -C eth0 adaptive-rx on</span><br></pre></td></tr></table></figure>
<p>除了直接开启这个功能，也可以实现其他更精细的控制：</p>
<ul>
<li><code>rx-usecs</code>：数据帧到达后，延迟多长时间产生中断信号，单位微妙</li>
<li><code>rx-frames</code>：触发中断前积累数据帧的最大个数</li>
<li><code>rx-usecs-irq</code>：如果有中断处理正在执行，当前中断延迟多久送达 CPU</li>
<li><code>rx-frames-irq</code>：如果有中断处理正在执行，最多积累多少个数据帧</li>
<li>等等</li>
</ul>
<p>参考：<a href="https://github.com/torvalds/linux/blob/v3.13/include/uapi/linux/ethtool.h#L184-L255" target="_blank" rel="noopener">include/uapi/linux/ethtool.h</a></p>
<blockquote>
<p>IRQ affinities</p>
</blockquote>
<p>支持网卡多队列的情况下，可以通过设置 CPU 亲和性提高数据本地性。具体来说就是，我们可以指定哪些中断交给哪些 CPU 处理。调整之前需要首先检查两个事情：</p>
<ul>
<li>是不是有守护进程 <code>irqbalance</code> 在运行？</li>
<li>检查 <code>/proc/interrupts</code> 获取每个网卡队列对应的中断号</li>
</ul>
<p>最后，我们修改系统文件 <code>/proc/irq/IRQ_NUMBER/smp_affinity</code> 来指定 CPU：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo bash -c <span class="string">'echo 1 &gt; /proc/irq/8/smp_affinity'</span></span><br></pre></td></tr></table></figure>
<p>其中的值为十六进制的位掩码，上面表示指定 CPU 0 处理中断号 8。</p>
<h3 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h3><p>一旦上述的软中断过程发现有待处理的软中断信号，就会开始处理并最终执行到 <code>net_rx_action</code></p>
<h4 id="net-rx-action-处理循环"><a href="#net-rx-action-处理循环" class="headerlink" title="net_rx_action 处理循环"></a><code>net_rx_action</code> 处理循环</h4><p>核心工作就是处理位于 DMA 内存区域的数据帧；遍历挂在当前 CPU 上的 NAPI 结构体列表，出队并逐个处理。整个循环会限制整体的工作量，以及驱动注册的 <code>poll</code> 函数的可执行时长，基于下面两点：</p>
<ol>
<li>随时检查工作量相关的 <code>budget</code></li>
<li>检查当前已经执行的时长</li>
</ol>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (!list_empty(&amp;sd-&gt;poll_list)) &#123;</span><br><span class="line">  struct napi_struct *n;</span><br><span class="line">  int work, weight;</span><br><span class="line"></span><br><span class="line">  /* If softirq window is exhausted <span class="keyword">then</span> punt.</span><br><span class="line">   * Allow this to run <span class="keyword">for</span> 2 jiffies since <span class="built_in">which</span> will allow</span><br><span class="line">   * an average latency of 1.5/HZ.</span><br><span class="line">   */</span><br><span class="line">  <span class="keyword">if</span> (unlikely(budget &lt;= 0 || time_after_eq(jiffies, time_limit)))</span><br><span class="line">    goto softnet_break;</span><br></pre></td></tr></table></figure>
<p>有了这样的限制，就可以阻止 CPU 被网络包处理独占。</p>
<p>重点是 <code>budget</code> 生效的范围：针对的是当前 CPU 上的所有 NAPI 结构体列表。如果机器的 CPU 数目不足以承担全部的网卡队列，就会导致多个队列只能由同一个 CPU 处理，消耗同一个 <code>budget</code>；这种情况下，可以考虑增大 <code>budget</code> 来更快的处理数据，虽然会增加 CPU 使用率的 <code>si</code> 部分，但是有助于降低数据帧处理的延时。</p>
<h4 id="NAPI-poll-函数和权重"><a href="#NAPI-poll-函数和权重" class="headerlink" title="NAPI poll 函数和权重"></a>NAPI <code>poll</code> 函数和权重</h4><p>之前提到每个 NAPI 结构体都赋予了一个权重，目前是硬编码为 64，现在看下这个值是如何起作用的：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">weight = n-&gt;weight;</span><br><span class="line"></span><br><span class="line">work = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">if</span> (test_bit(NAPI_STATE_SCHED, &amp;n-&gt;state)) &#123;</span><br><span class="line">        work = n-&gt;poll(n, weight);</span><br><span class="line">        trace_napi_poll(n);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">WARN_ON_ONCE(work &gt; weight);</span><br><span class="line"></span><br><span class="line">budget -= work;</span><br></pre></td></tr></table></figure>
<p>权重值被传递给 <code>poll</code> 函数，作为处理数据帧的上限，而实际的处理量通过返回值保存在 <code>work</code> 变量中，并最终从 <code>budget</code> 里扣除。</p>
<p>假设：</p>
<ol>
<li>驱动注册的权重为 64（这个驱动硬编码进去的）</li>
<li><code>budget</code> 保持默认值 300</li>
</ol>
<p>那么整个软中断处理会在下面两种情况下停止：</p>
<ul>
<li><code>poll</code> 函数最多被调用五次（如果没有更多数据，可能用不了五次）</li>
<li>时长达到或者超过了 2 jiffies</li>
</ul>
<h4 id="NAPI-和网卡驱动的约定"><a href="#NAPI-和网卡驱动的约定" class="headerlink" title="NAPI 和网卡驱动的约定"></a>NAPI 和网卡驱动的约定</h4><p>关于这两者之前的约定，很重要的一点是：什么情况下关闭 NAPI？</p>
<ul>
<li>如果驱动的 <code>poll</code> 函数用完了权重 <code>weight</code>，那么不能更改 NAPI 状态，交给 <code>net_rx_action</code> 处理</li>
<li>如果没有用完，驱动必须关闭 NAPI，等待下一次硬中断触发</li>
</ul>
<h4 id="结束-net-rx-action-循环"><a href="#结束-net-rx-action-循环" class="headerlink" title="结束 net_rx_action 循环"></a>结束 <code>net_rx_action</code> 循环</h4><p>结束处理循环之前，首先就是处理上述约定的第一种情况：把 NAPI 结构体移动到当前 CPU 的队尾，这样就可以让 CPU 紧接着处理队列上的下一个 NAPI 结构体。</p>
<p>达到循环结束的限制条件之后，会跳转到下面的地方执行：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">softnet_break:</span><br><span class="line">  sd-&gt;time_squeeze++;</span><br><span class="line">  __raise_softirq_irqoff(NET_RX_SOFTIRQ);</span><br><span class="line">  <span class="keyword">goto</span> out;</span><br></pre></td></tr></table></figure>
<p>结构体 <code>softnet_data</code> 递增了统计数据 <code>time_squeeze</code>，并关闭了软中断 <code>NET_RX_SOFTIRQ</code>。</p>
<p>这里的 <code>time_squeeze</code> 衡量的是 <code>net_rx_action</code> 被迫中断的次数；虽然有更多的数据帧等待处理，但是已经耗尽了 <code>buddget</code> 或者指定的时长。这个指标可以用来发现网络处理的瓶颈。</p>
<p>关闭软中断可以理解，因为 CPU 需要释放给用户级程序使用，而不能频繁用在中断处理上。</p>
<p>程序最终跳转到 <code>out</code> 标签处。其实不止被限制的情况下会执行到这里，还有就是 <code>budget</code> 多于等待处理的数据帧，当前 CPU 已经没有等待处理的 NAPI 结构体了，也会执行到 <code>out</code> 标签。</p>
<p><code>out</code> 标签中需要提到的一个重要步骤是 <code>net_rps_action_and_irq_enable</code>，这个函数会唤醒其他 CPU 来处理他们本地的数据帧，这个和 RPS 功能有关，后续会详细说明。</p>
<h4 id="NAPI-poll-函数详解"><a href="#NAPI-poll-函数详解" class="headerlink" title="NAPI poll 函数详解"></a>NAPI <code>poll</code> 函数详解</h4><p>前面提到过，驱动程序负责分配内存空间给 DMA 写入数据帧，所以驱动程序也需要负责 <code>unmap</code> 这些内核空间，处理数据帧并向协议栈上层传递。</p>
<p>作为例子，我们看下 <code>igb</code> 的实现：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *  igb_poll - NAPI Rx polling callback</span></span><br><span class="line"><span class="comment"> *  @napi: napi polling structure</span></span><br><span class="line"><span class="comment"> *  @budget: count of how many packets we should handle</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">igb_poll</span><span class="params">(struct napi_struct *napi, <span class="keyword">int</span> budget)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">        <span class="class"><span class="keyword">struct</span> <span class="title">igb_q_vector</span> *<span class="title">q_vector</span> = <span class="title">container_of</span>(<span class="title">napi</span>,</span></span><br><span class="line"><span class="class">                                                     <span class="title">struct</span> <span class="title">igb_q_vector</span>,</span></span><br><span class="line"><span class="class">                                                     <span class="title">napi</span>);</span></span><br><span class="line">        <span class="keyword">bool</span> clean_complete = <span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> CONFIG_IGB_DCA</span></span><br><span class="line">        <span class="keyword">if</span> (q_vector-&gt;adapter-&gt;flags &amp; IGB_FLAG_DCA_ENABLED)</span><br><span class="line">                igb_update_dca(q_vector);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"></span><br><span class="line">        <span class="comment">/* ... */</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (q_vector-&gt;rx.ring)</span><br><span class="line">                clean_complete &amp;= igb_clean_rx_irq(q_vector, budget);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* If all work not completed, return budget and keep polling */</span></span><br><span class="line">        <span class="keyword">if</span> (!clean_complete)</span><br><span class="line">                <span class="keyword">return</span> budget;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* If not enough Rx work done, exit the polling mode */</span></span><br><span class="line">        napi_complete(napi);</span><br><span class="line">        igb_ring_irq_enable(q_vector);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里需要关注的几个操作：</p>
<ul>
<li>如果内核支持 DCA 的话，这里会预热 CPU 缓存，这样后续访问就可以直接命中</li>
<li><code>igb_clean_rx_irq</code> 处理大部分核心工作，后续详细说明</li>
<li>检查是否有更多的数据帧需要处理，如果有，返回 <code>budget</code> 并持续轮询；</li>
<li>如果没有，则通过 <code>napi_complete</code> 关闭 NAPI 子系统，并解除中断屏蔽</li>
</ul>
<p><code>igb_clean_rx_irq</code></p>
<p>这个函数实现为一个循环，一次处理一个数据帧，直到耗尽 <code>budget</code> 或者没有更多数据帧等待处理。</p>
<ol>
<li>分配新的 buffer 加入到 ring buffer（接收队列 - RX queue）</li>
<li>从接收队列中取走一个 buffer，加入到 <code>skb</code> 结构体</li>
<li>检查该 buffer 是不是 <code>End of Packet</code>，如果不是则继续取下一个 buffer 追加到 <code>skb</code></li>
<li>校验整个数据帧的布局和头部是否完整</li>
<li>更新统计数据：已处理的字节数（<code>skb-&gt;len</code>）</li>
<li>设置 <code>skb</code> 结构体的 哈希值，校验和，时间戳，协议簇 等，这些值都是硬件提供的；如果硬件报告了校验和失败，这里会更新对应的统计数据 <code>csum_error</code>，继续交给上层协议处理；协议字段通过一个单独的函数 <code>eth_type_trans</code> 计算处理，并保存在 <code>skb</code> 结构体里</li>
<li>构造完毕的 <code>skb</code> 结构体通过 <code>napi_gro_receive</code> 向协议栈上层传递</li>
<li>更新统计数据：已处理的包数量</li>
<li>重复上述步骤，处理下一个数据帧</li>
</ol>
<h4 id="监控数据处理"><a href="#监控数据处理" class="headerlink" title="监控数据处理"></a>监控数据处理</h4><p>上述过程提供的统计数据一般都输出到了系统文件：<code>/proc/net/softnet_stat</code>，但是关于这个文件的文档说明几乎没有；文件里的每个字段也没有对应的标签分类，不能的内核版本都可能不一样。目前只能通过内核源码来明确具体含义：</p>
<p><a href="https://github.com/torvalds/linux/blob/v3.13/net/core/net-procfs.c#L161-L165" target="_blank" rel="noopener">net/core/net-procfs.c</a></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">seq_printf(seq,</span><br><span class="line">     <span class="string">"%08x %08x %08x %08x %08x %08x %08x %08x %08x %08x %08x\n"</span>,</span><br><span class="line">     sd-&gt;processed, sd-&gt;dropped, sd-&gt;time_squeeze, <span class="number">0</span>,</span><br><span class="line">     <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="comment">/* was fastroute */</span></span><br><span class="line">     sd-&gt;cpu_collision, sd-&gt;received_rps, flow_limit_count);</span><br></pre></td></tr></table></figure>
<p>之前在 <code>net_rx_action</code> 里提到了 <code>squeeze_time</code> 这个统计数据，刚好这里一起看下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ cat /proc/net/softnet_stat</span><br><span class="line">6dcad223 00000000 00000001 00000000 00000000 00000000 00000000 00000000 00000000 00000000</span><br><span class="line">6f0e1565 00000000 00000002 00000000 00000000 00000000 00000000 00000000 00000000 00000000</span><br><span class="line">660774ec 00000000 00000003 00000000 00000000 00000000 00000000 00000000 00000000 00000000</span><br><span class="line">61c99331 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000</span><br><span class="line">6794b1b3 00000000 00000005 00000000 00000000 00000000 00000000 00000000 00000000 00000000</span><br><span class="line">6488cb92 00000000 00000001 00000000 00000000 00000000 00000000 00000000 00000000 00000000</span><br></pre></td></tr></table></figure>
<p>关于这个文件的的几点解释：</p>
<ul>
<li>每一行对应一个 <code>struct softnet_data</code> 结构体，也就对应一个 CPU</li>
<li>数值之间空格分割，输出格式是 十六进制</li>
<li>第一个值 <code>sd-&gt;processed</code> 表示处理的包数量（多网卡 bond 模式可能多于实际的收包数量）</li>
<li>第二个值 <code>sd-&gt;dropped</code> 表示丢包数量，因为处理队列满了</li>
<li>第三个值 <code>sd-&gt;time_squeeze</code> 表示软中断处理 <code>net_rx_action</code> 被迫打断的次数</li>
<li>接下来的五个值都是零</li>
<li>第九个值 <code>sd-&gt;cpu_collision</code> 表示发送数据时获取设备锁冲突，比如多个 CPU 同时发送数据</li>
<li>第十个值 <code>sd-&gt;received_rps</code> 表示当前 CPU 被唤醒的次数（通过处理器间中断）</li>
<li>最后一个值 <code>sd-&gt;flow_limit_count</code> 表示 <code>flow limit</code> 达到上限的次数</li>
</ul>
<h4 id="调优数据处理"><a href="#调优数据处理" class="headerlink" title="调优数据处理"></a>调优数据处理</h4><p>这里可以调整的一个参数是 <code>budget</code>，影响 <code>net_rx_action</code> 的处理效率。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo sysctl -w net.core.netdev_budget=600</span><br></pre></td></tr></table></figure>
<p>这个值针对每个 CPU 单独生效，默认值是 300</p>
<h3 id="GRO"><a href="#GRO" class="headerlink" title="GRO"></a>GRO</h3><p>Generic Receive Offloading (GRO) 是一种替代硬件 Large Receive Offloading (LRO) 的软件实现。背后的主要思想是，尽量减少向上传递的包数量，进而减少上层协议栈处理的 CPU 负担。举个例子，在传输一个大文件的时候，大部分的数据帧都是文件数据块；相较于一次传递一个小数据帧，如果可以把这些数据帧组合成一个巨大的数据帧传递给上层，那么协议层就只需要处理一个协议头，间接提高用户程序的数据接收速度。</p>
<p>这种合并包的优化存在一个问题就是，潜在的信息丢失。比如有些数据帧设置了额外的选项，或者其他什么标志位，在被合并到其他数据帧里的时候就可能丢失这些信息。这也是 LRO 没有大范围应用的原因，因为硬件层的实现没有严格限制包合并的条件和规则。GRO 作为纯软件实现，则严格规定了什么情况下才能对数据包做合并。</p>
<p>PS：我们偶尔在 <code>tcpdump</code> 抓包的时候，可能会看到一些巨大的包尺寸，这就是系统开启了 GRO 的结果；因为从整个网络栈来看，抓包的过程发生在 GRO 完成之后。</p>
<p>这个特性可以通过 <code>ethtool</code> 工具进行调整：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ethtool -K eth0 gro on</span><br></pre></td></tr></table></figure>
<p>也可以查看当前系统是否开启了这个功能：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ethtool -k eth0 | grep generic-receive-offload</span><br><span class="line">generic-receive-offload: on</span><br></pre></td></tr></table></figure>
<p>需要注意的是，这个操作会导致网卡重启，长连接会被中断。</p>
<h3 id="napi-gro-receive"><a href="#napi-gro-receive" class="headerlink" title="napi_gro_receive"></a><code>napi_gro_receive</code></h3><p>实现上，对于 GRO 开启的情况，会逐个遍历上层协议栈注册到 GRO 的 <code>filter</code>。通过这样的方式，协议层就可以告知设备层，这个数据包是否可以被合并（比如判断是否属于同一个网络流），以及合并以后需要做什么样的额外操作（比如 tcp 需要对一个被合并的包发送给对端 ack 响应）。细节略。</p>
<h2 id="内部机制：负载均衡和流控"><a href="#内部机制：负载均衡和流控" class="headerlink" title="内部机制：负载均衡和流控"></a>内部机制：负载均衡和流控</h2><ul>
<li>RSS / RPS</li>
<li>RFS / aRFS</li>
</ul>
<h3 id="Receive-Packet-Steering-RPS"><a href="#Receive-Packet-Steering-RPS" class="headerlink" title="Receive Packet Steering (RPS)"></a>Receive Packet Steering (RPS)</h3><p>一个 CPU 会同时处理硬件中断和对数据包的轮询处理。大部分现代网卡都支持多队列，也就是说，收到的数据帧可以被 DMA 拷贝到不同的内存区域，每个队列对应一份独立的空间，也就对应一个独立的 NAPI 结构体来管理这个区域。因此，可以利用到多个 CPU 来处理网络数据帧。</p>
<p>这个功能一般叫做 Receive Side Scaling (RSS)。</p>
<p>Receive Packet Steering (RPS)则是对 RSS 的软件实现，因此可以针对任意类型网卡开启，即使只有一个队列。因为是纯软件实现，RPS 起作用之前，数据帧已经被从 DMA 区域取走了，所以只能影响这个时间点之后的协议栈负载。</p>
<p>RPS 基本的流程是这样的：</p>
<ol>
<li>基于数据帧产生一个哈希值，进而计算出目标 CPU；</li>
<li>然后数据帧被加入到目标 CPU 的网络接收队列（backlog）</li>
<li>发送 Inter-processor Interrupt (IPI) 到目标 CPU</li>
<li>唤醒目标 CPU 来处理它的 backlog 队列</li>
</ol>
<p>之前提到过，<code>/proc/net/softnet_stat</code> 文件中包含了一个相关的统计项 <code>sd-&gt;received_rps</code>，记录了 CPU 收到的 IPI 中断的数量。</p>
<p>所以数据帧在递交上层协议栈之前，可能会首先经过 RPS 被负载均衡到其他 CPU，从而分散处理压力。</p>
<h3 id="调优：启用-RPS"><a href="#调优：启用-RPS" class="headerlink" title="调优：启用 RPS"></a>调优：启用 RPS</h3><p>通过位掩码，我们可以指定 CPU 来处理指定的网卡队列：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/sys/class/net/eth0/queues/rx-0/rps_cpus</span><br></pre></td></tr></table></figure>
<p>上面的文件指向网卡 <code>eth0</code> 的 <code>rx-0</code> 队列，格式为十六进制的数字，可以指定一个或多个 CPU。对 RSS 网卡来说，这个功能没有开启的必要；只有在队列数据少于 CPU 数目时，可以考虑开启 RPS，并且尽量保证，被均衡的 CPU 和当前处理中断的 CPU 共用同一个 CPU cache。</p>
<h3 id="Receive-Flow-Steering-RFS"><a href="#Receive-Flow-Steering-RFS" class="headerlink" title="Receive Flow Steering (RFS)"></a>Receive Flow Steering (RFS)</h3><p>RFS 一般结合 RPS 一起使用，因为 RPS 没有考虑到数据本地性的问题，缓存命中率偏低。利用 RFS，我们可以把属于同一个流的数据帧，尽量引导到同一个 CPU 进行处理。</p>
<h3 id="调优：开启-RFS"><a href="#调优：开启-RFS" class="headerlink" title="调优：开启 RFS"></a>调优：开启 RFS</h3><p>RFS 依赖于 RPS，必须首先保证 RPS 是开启的。</p>
<p>RFS 内部维护一个流的全局哈希表，这个哈希表的大小可以调整：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo sysctl -w net.core.rps_sock_flow_entries=32768</span><br></pre></td></tr></table></figure>
<p>另外，针对每个网卡队列，也可以单独设置 rps 的流数量：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo bash -c <span class="string">'echo 2048 &gt; /sys/class/net/eth0/queues/rx-0/rps_flow_cnt'</span></span><br></pre></td></tr></table></figure>
<h2 id="数据包递交上层协议栈：netif-receive-skb"><a href="#数据包递交上层协议栈：netif-receive-skb" class="headerlink" title="数据包递交上层协议栈：netif_receive_skb"></a>数据包递交上层协议栈：<code>netif_receive_skb</code></h2><ol>
<li>timestamp - 涉及负载均衡</li>
<li>RPS - 默认不开启<ol>
<li>插入 backlog / 执行 flow limit</li>
<li>backlog NAPI poller - <code>process_backlog</code></li>
</ol>
</li>
<li>packet cap - tcpdump 等工具生效的阶段</li>
<li>protocol layer</li>
</ol>
<h3 id="调优：时间戳"><a href="#调优：时间戳" class="headerlink" title="调优：时间戳"></a>调优：时间戳</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo sysctl -w net.core.netdev_tstamp_prequeue=0</span><br></pre></td></tr></table></figure>
<p>这里可以调整数据帧生成时间戳的时机，默认是 1，也就是在 RPS 之前；设置为 0 的话，可以把生成时间戳的负载均衡到其他 CPU 上（会引入部分延时，毕竟数据在 CPU 之间移动了）。</p>
<h3 id="backlog-队列"><a href="#backlog-队列" class="headerlink" title="backlog 队列"></a>backlog 队列</h3><p>RPS 首先计算目标 CPU，然后将数据加入目标 CPU 的 backlog 队列：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cpu = get_rps_cpu(skb-&gt;dev, skb, &amp;rflow);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (cpu &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">  ret = enqueue_to_backlog(skb, cpu, &amp;rflow-&gt;last_qtail);</span><br><span class="line">  rcu_read_unlock();</span><br><span class="line">  <span class="keyword">return</span> ret;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在 <code>enqueue_to_backlog</code> 中，会检查该 CPU 的队列大小：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">qlen = skb_queue_len(&amp;sd-&gt;input_pkt_queue);</span><br><span class="line"><span class="keyword">if</span> (qlen &lt;= netdev_max_backlog &amp;&amp; !skb_flow_limit(skb, qlen)) &#123;</span><br></pre></td></tr></table></figure>
<p><code>input_pkt_queue</code> 代表的就是 cpu 的 backlog 队列，用来和 <code>netdev_max_backlog</code> 比较，如果队列过长，数据帧就会被丢弃；另外，如果 <code>flow limit</code> 生效，也会导致丢包。这里的丢包数据可以在 <code>/proc/net/softnet_stat</code> 文件中看到。</p>
<p>PS：如果没有开启 RPS，这里的限制条件就没有用到了。</p>
<h3 id="Flow-limits"><a href="#Flow-limits" class="headerlink" title="Flow limits"></a>Flow limits</h3><p>可能出现的情况是，某个流的数据巨多，挤满了 backlog 队列，进而导致其他数据流的处理被延迟；所以这里还需要进行流控，避免某个流独占 CPU。这里实现的基本原理是，在 backlog 队列达到最大值的一半时，开始检查每个流占用队列的比例，如果当前队列中某个流的数据帧超过了一半，就对当前流执行丢包。</p>
<h3 id="监控-backlog-丢包"><a href="#监控-backlog-丢包" class="headerlink" title="监控 backlog 丢包"></a>监控 backlog 丢包</h3><h3 id="调优"><a href="#调优" class="headerlink" title="调优"></a>调优</h3><p>调整 backlog 队列的最大值，可以减少丢包的发生：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo sysctl -w net.core.netdev_max_backlog=3000</span><br></pre></td></tr></table></figure>
<p>调整 backlog 队列处理的权重：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo sysctl -w net.core.dev_weight=600</span><br></pre></td></tr></table></figure>
<p>这个值和驱动注册的 weight 类似，但是可以调整（驱动一般硬编码为 64）。</p>
<p>流控所使用的哈希表大小也可以调整：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo sysctl -w net.core.flow_limit_table_len=8192</span><br></pre></td></tr></table></figure>
<p>这个哈希表越大，流控的粒度越细；相反，则容易误伤。</p>
<h3 id="抓包"><a href="#抓包" class="headerlink" title="抓包"></a>抓包</h3><p>在把数据帧递交上层协议栈的最后关头，会经过 <code>packet tap</code>，执行一系列过滤操作。</p>
<h3 id="送达协议层"><a href="#送达协议层" class="headerlink" title="送达协议层"></a>送达协议层</h3><p>解析数据帧中的协议类型字段，每种协议类型都注册了各自的接收函数，从而可以把数据帧发送给特定的协议层。</p>
<h2 id="协议层"><a href="#协议层" class="headerlink" title="协议层"></a>协议层</h2><ul>
<li>IP 层</li>
<li>应用层协议注册</li>
<li>UDP 层</li>
</ul>
<h2 id="MISC"><a href="#MISC" class="headerlink" title="MISC"></a>MISC</h2><h3 id="SO-INCOMING-CPU"><a href="#SO-INCOMING-CPU" class="headerlink" title="SO_INCOMING_CPU"></a><code>SO_INCOMING_CPU</code></h3><ul>
<li>作用和使用场景</li>
<li>使用内核版本</li>
</ul>
<p>参考：<a href="http://man7.org/linux/man-pages/man7/socket.7.html" target="_blank" rel="noopener">http://man7.org/linux/man-pages/man7/socket.7.html</a></p>
<p>gettable since Linux 3.19, settable since Linux 4.4</p>
<p>Sets or gets the CPU affinity of a socket.  Expects an integer flag.</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> cpu = <span class="number">1</span>;</span><br><span class="line">setsockopt(fd, SOL_SOCKET, SO_INCOMING_CPU, &amp;cpu, <span class="keyword">sizeof</span>(cpu));</span><br></pre></td></tr></table></figure>
<p>Because all of the packets for a single stream (i.e., all packets for the same 4-tuple) arrive on the single RX queue that is associated with a particular CPU, the typical use case is to employ one listening process per RX queue, with the incoming flow being handled by a listener on the same CPU that is handling the RX queue.  This provides optimal NUMA behavior and keeps CPU caches hot.</p>
<p>目前最新版本内核已经弃用，参考这篇 <a href="https://blog.cloudflare.com/perfect-locality-and-three-epic-systemtap-scripts/" target="_blank" rel="noopener">cloudflare blogpost</a></p>

      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/03/30/hello-world/" rel="next" title="Hello World">
                <i class="fa fa-chevron-left"></i> Hello World
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">linuxholic</p>
              <div class="site-description motion-element" itemprop="description"></div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">2</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              

              
            </nav>
          

          

          

          

          

          
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#主题"><span class="nav-number">1.</span> <span class="nav-text">主题</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#基本原则"><span class="nav-number">2.</span> <span class="nav-text">基本原则</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#概述"><span class="nav-number">3.</span> <span class="nav-text">概述</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#详述"><span class="nav-number">4.</span> <span class="nav-text">详述</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#网卡驱动"><span class="nav-number">4.1.</span> <span class="nav-text">网卡驱动</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#PCI-注册"><span class="nav-number">4.1.1.</span> <span class="nav-text">PCI 注册</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#设备初始化"><span class="nav-number">4.1.2.</span> <span class="nav-text">设备初始化</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#NOTE：IRQ-和-NAPI"><span class="nav-number">4.1.2.1.</span> <span class="nav-text">NOTE：IRQ 和 NAPI</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#网卡启动"><span class="nav-number">4.1.3.</span> <span class="nav-text">网卡启动</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#网卡监控"><span class="nav-number">4.1.4.</span> <span class="nav-text">网卡监控</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#网卡调优"><span class="nav-number">4.1.5.</span> <span class="nav-text">网卡调优</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#软中断"><span class="nav-number">4.2.</span> <span class="nav-text">软中断</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Linux-网络栈"><span class="nav-number">4.3.</span> <span class="nav-text">Linux 网络栈</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#初始化"><span class="nav-number">4.3.1.</span> <span class="nav-text">初始化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据接收"><span class="nav-number">4.3.2.</span> <span class="nav-text">数据接收</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#中断处理函数"><span class="nav-number">4.3.2.1.</span> <span class="nav-text">中断处理函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#NAPI-napi-schedule"><span class="nav-number">4.3.2.2.</span> <span class="nav-text">NAPI napi_schedule</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#监控数据接收"><span class="nav-number">4.3.2.3.</span> <span class="nav-text">监控数据接收</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#调优数据接收"><span class="nav-number">4.3.2.4.</span> <span class="nav-text">调优数据接收</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据处理"><span class="nav-number">4.3.3.</span> <span class="nav-text">数据处理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#net-rx-action-处理循环"><span class="nav-number">4.3.3.1.</span> <span class="nav-text">net_rx_action 处理循环</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#NAPI-poll-函数和权重"><span class="nav-number">4.3.3.2.</span> <span class="nav-text">NAPI poll 函数和权重</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#NAPI-和网卡驱动的约定"><span class="nav-number">4.3.3.3.</span> <span class="nav-text">NAPI 和网卡驱动的约定</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#结束-net-rx-action-循环"><span class="nav-number">4.3.3.4.</span> <span class="nav-text">结束 net_rx_action 循环</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#NAPI-poll-函数详解"><span class="nav-number">4.3.3.5.</span> <span class="nav-text">NAPI poll 函数详解</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#监控数据处理"><span class="nav-number">4.3.3.6.</span> <span class="nav-text">监控数据处理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#调优数据处理"><span class="nav-number">4.3.3.7.</span> <span class="nav-text">调优数据处理</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GRO"><span class="nav-number">4.3.4.</span> <span class="nav-text">GRO</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#napi-gro-receive"><span class="nav-number">4.3.5.</span> <span class="nav-text">napi_gro_receive</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#内部机制：负载均衡和流控"><span class="nav-number">4.4.</span> <span class="nav-text">内部机制：负载均衡和流控</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Receive-Packet-Steering-RPS"><span class="nav-number">4.4.1.</span> <span class="nav-text">Receive Packet Steering (RPS)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#调优：启用-RPS"><span class="nav-number">4.4.2.</span> <span class="nav-text">调优：启用 RPS</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Receive-Flow-Steering-RFS"><span class="nav-number">4.4.3.</span> <span class="nav-text">Receive Flow Steering (RFS)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#调优：开启-RFS"><span class="nav-number">4.4.4.</span> <span class="nav-text">调优：开启 RFS</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#数据包递交上层协议栈：netif-receive-skb"><span class="nav-number">4.5.</span> <span class="nav-text">数据包递交上层协议栈：netif_receive_skb</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#调优：时间戳"><span class="nav-number">4.5.1.</span> <span class="nav-text">调优：时间戳</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#backlog-队列"><span class="nav-number">4.5.2.</span> <span class="nav-text">backlog 队列</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Flow-limits"><span class="nav-number">4.5.3.</span> <span class="nav-text">Flow limits</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#监控-backlog-丢包"><span class="nav-number">4.5.4.</span> <span class="nav-text">监控 backlog 丢包</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#调优"><span class="nav-number">4.5.5.</span> <span class="nav-text">调优</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#抓包"><span class="nav-number">4.5.6.</span> <span class="nav-text">抓包</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#送达协议层"><span class="nav-number">4.5.7.</span> <span class="nav-text">送达协议层</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#协议层"><span class="nav-number">4.6.</span> <span class="nav-text">协议层</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MISC"><span class="nav-number">4.7.</span> <span class="nav-text">MISC</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#SO-INCOMING-CPU"><span class="nav-number">4.7.1.</span> <span class="nav-text">SO_INCOMING_CPU</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">linuxholic</span>

  

  
</div>


  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> v7.0.1</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/src/utils.js?v=7.0.1"></script>

  <script src="/js/src/motion.js?v=7.0.1"></script>



  
  


  <script src="/js/src/schemes/muse.js?v=7.0.1"></script>



  
  <script src="/js/src/scrollspy.js?v=7.0.1"></script>
<script src="/js/src/post-details.js?v=7.0.1"></script>



  


  <script src="/js/src/next-boot.js?v=7.0.1"></script>


  

  

  

  


  


  




  

  

  

  

  

  

  

  

  

  

  

  

  

  

</body>
</html>
